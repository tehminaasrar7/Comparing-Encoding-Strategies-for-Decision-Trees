{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8656425,"sourceType":"datasetVersion","datasetId":5185839}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Create a Sample Dataset\nCreate a synthetic dataset with some categorical and numerical columns and save it as a CSV file.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a sample dataset\ndata = {\n    'Category': np.random.choice(['A', 'B', 'C', 'D'], size=100),\n    'Subcategory': np.random.choice(['X', 'Y', 'Z'], size=100),\n    'NumericalFeature1': np.random.randn(100),\n    'NumericalFeature2': np.random.rand(100) * 100,\n    'Target': np.random.choice([0, 1], size=100)\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Save DataFrame to CSV\ndf.to_csv('sample_dataset.csv', index=False)\n\n# Display the first few rows of the dataset\nprint(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:34:21.864894Z","iopub.execute_input":"2024-06-10T13:34:21.865327Z","iopub.status.idle":"2024-06-10T13:34:23.097059Z","shell.execute_reply.started":"2024-06-10T13:34:21.865291Z","shell.execute_reply":"2024-06-10T13:34:23.095904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Load and Explore the Dataset\nNow, assuming  uploaded sample_dataset.csv to  Kaggle environment, let's load and explore the dataset.\n","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\n\n# Load dataset\ndata = pd.read_csv('/kaggle/input/sample-dataset-csv/sample_dataset.csv')\n\n# Display the first few rows of the dataset\nprint(data.head())\n\n# Identify categorical columns\ncategorical_columns = ['Category', 'Subcategory']\n\n# Display unique values and their counts for each categorical column\nfor col in categorical_columns:\n    print(f\"Column: {col}\")\n    print(data[col].value_counts())\n    print(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:45:37.950213Z","iopub.execute_input":"2024-06-10T13:45:37.950591Z","iopub.status.idle":"2024-06-10T13:45:37.980730Z","shell.execute_reply.started":"2024-06-10T13:45:37.950563Z","shell.execute_reply":"2024-06-10T13:45:37.979465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Preprocessing - One-Hot Encoding and Label Encoding\nAfter loading and exploring the dataset, the next step involves preprocessing the categorical variables using one-hot encoding and label encoding.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# One-hot encoding\nonehot_encoder = OneHotEncoder(sparse_output=False)  # Adjusted argument name\nonehot_encoded = onehot_encoder.fit_transform(data[['Category', 'Subcategory']])\n\n# Create a DataFrame with one-hot encoded columns\nonehot_encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(['Category', 'Subcategory']))\nprint(\"One-Hot Encoded DataFrame:\")\nprint(onehot_encoded_df.head())\n\n# Label encoding\nlabel_encoder = LabelEncoder()\ndata['Category_LabelEncoded'] = label_encoder.fit_transform(data['Category'])\ndata['Subcategory_LabelEncoded'] = label_encoder.fit_transform(data['Subcategory'])\n\nprint(\"\\nLabel Encoded Columns:\")\nprint(data[['Category', 'Category_LabelEncoded', 'Subcategory', 'Subcategory_LabelEncoded']].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:49:24.067266Z","iopub.execute_input":"2024-06-10T13:49:24.068348Z","iopub.status.idle":"2024-06-10T13:49:24.091015Z","shell.execute_reply.started":"2024-06-10T13:49:24.068309Z","shell.execute_reply":"2024-06-10T13:49:24.089759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Explanation:\n**OneHotEncode**r: \nAdjusted the argument sparse to sparse_output which is compatible with the latest version of sklearn.\n**get_feature_names_ou**t: \nEnsures that the column names are correctly extracted after one-hot encoding.","metadata":{}},{"cell_type":"markdown","source":"# Step 4: Building and Evaluating Decision Tree Models\nIn this step, we will:\n\n* Split the data into training and testing sets.\n* Train decision tree models using one-hot encoded data and label encoded data.\n* Evaluate and compare the models' performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Define features and target\nX_onehot = onehot_encoded_df\nX_label = data[['Category_LabelEncoded', 'Subcategory_LabelEncoded', 'NumericalFeature1', 'NumericalFeature2']]\ny = data['Target']\n\n# Split data into training and testing sets\nX_train_onehot, X_test_onehot, y_train, y_test = train_test_split(X_onehot, y, test_size=0.3, random_state=42)\nX_train_label, X_test_label, y_train, y_test = train_test_split(X_label, y, test_size=0.3, random_state=42)\n\n# Train decision tree classifier on one-hot encoded data\nclf_onehot = DecisionTreeClassifier(random_state=42)\nclf_onehot.fit(X_train_onehot, y_train)\n\n# Train decision tree classifier on label encoded data\nclf_label = DecisionTreeClassifier(random_state=42)\nclf_label.fit(X_train_label, y_train)\n\n# Make predictions\npredictions_onehot = clf_onehot.predict(X_test_onehot)\npredictions_label = clf_label.predict(X_test_label)\n\n# Evaluate models\naccuracy_onehot = accuracy_score(y_test, predictions_onehot)\naccuracy_label = accuracy_score(y_test, predictions_label)\n\nprint(f'Accuracy with One-Hot Encoding: {accuracy_onehot:.4f}')\nprint(f'Accuracy with Label Encoding: {accuracy_label:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T13:53:19.865374Z","iopub.execute_input":"2024-06-10T13:53:19.865832Z","iopub.status.idle":"2024-06-10T13:53:20.187759Z","shell.execute_reply.started":"2024-06-10T13:53:19.865800Z","shell.execute_reply":"2024-06-10T13:53:20.186612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explanation:\n**Splitting the Data**: The data is split into training and testing sets using train_test_split.\n**Training the Models**: \nTwo decision tree models are trained, one on the one-hot encoded data and the other on the label encoded data.\n**Making Predictions:** \nThe models make predictions on the testing set.\nEvaluating the Models: The accuracy of each model is calculated and displayed.","metadata":{}},{"cell_type":"markdown","source":"# Step 5: Model Tuning and Further Evaluation\n\nTo potentially improve model performance, we can:\n\nTune the hyperparameters of the decision tree models.\nEvaluate the models using cross-validation.\nAnalyze feature importance to understand the impact of categorical features.\n\n# 5.1 Hyperparameter Tuning\nWe can use GridSearchCV to find the best hyperparameters for our decision tree models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'max_depth': [3, 5, 7, 10, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid search for one-hot encoded data\ngrid_search_onehot = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\ngrid_search_onehot.fit(X_train_onehot, y_train)\nbest_onehot = grid_search_onehot.best_estimator_\n\n# Grid search for label encoded data\ngrid_search_label = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\ngrid_search_label.fit(X_train_label, y_train)\nbest_label = grid_search_label.best_estimator_\n\n# Evaluate tuned models\npredictions_best_onehot = best_onehot.predict(X_test_onehot)\npredictions_best_label = best_label.predict(X_test_label)\n\naccuracy_best_onehot = accuracy_score(y_test, predictions_best_onehot)\naccuracy_best_label = accuracy_score(y_test, predictions_best_label)\n\nprint(f'Best Accuracy with One-Hot Encoding: {accuracy_best_onehot:.4f}')\nprint(f'Best Accuracy with Label Encoding: {accuracy_best_label:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:00:59.251429Z","iopub.execute_input":"2024-06-10T14:00:59.253129Z","iopub.status.idle":"2024-06-10T14:01:02.604210Z","shell.execute_reply.started":"2024-06-10T14:00:59.253054Z","shell.execute_reply":"2024-06-10T14:01:02.602821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.2 Cross-Validation\nTo get a more robust estimate of model performance, use cross-validation.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Cross-validation for one-hot encoded data\ncv_scores_onehot = cross_val_score(best_onehot, X_onehot, y, cv=5)\ncv_mean_onehot = cv_scores_onehot.mean()\n\n# Cross-validation for label encoded data\ncv_scores_label = cross_val_score(best_label, X_label, y, cv=5)\ncv_mean_label = cv_scores_label.mean()\n\nprint(f'Cross-Validation Accuracy with One-Hot Encoding: {cv_mean_onehot:.4f}')\nprint(f'Cross-Validation Accuracy with Label Encoding: {cv_mean_label:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:02:01.532700Z","iopub.execute_input":"2024-06-10T14:02:01.533509Z","iopub.status.idle":"2024-06-10T14:02:01.611993Z","shell.execute_reply.started":"2024-06-10T14:02:01.533440Z","shell.execute_reply":"2024-06-10T14:02:01.610678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.3 Feature Importance Analysis\nUnderstanding which features are most important can provide insights into the data and model behavior.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Feature importance for one-hot encoded data\nfeature_importances_onehot = best_onehot.feature_importances_\nfeature_names_onehot = onehot_encoder.get_feature_names_out(['Category', 'Subcategory'])  # Only pass the categorical features used for one-hot encoding\nplt.figure(figsize=(10, 5))\nplt.barh(feature_names_onehot, feature_importances_onehot)\nplt.title('Feature Importance (One-Hot Encoding)')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.show()\n\n# Feature importance for label encoded data\nfeature_importances_label = best_label.feature_importances_\nfeature_names_label = ['Category_LabelEncoded', 'Subcategory_LabelEncoded', 'NumericalFeature1', 'NumericalFeature2']\nplt.figure(figsize=(10, 5))\nplt.barh(feature_names_label, feature_importances_label)\nplt.title('Feature Importance (Label Encoding)')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:05:03.777143Z","iopub.execute_input":"2024-06-10T14:05:03.777629Z","iopub.status.idle":"2024-06-10T14:05:04.631527Z","shell.execute_reply.started":"2024-06-10T14:05:03.777595Z","shell.execute_reply":"2024-06-10T14:05:04.630293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Feature importance for one-hot encoded data\nfeature_importances_onehot = best_onehot.feature_importances_\nfeature_names_onehot = onehot_encoder.get_feature_names_out(['Category', 'Subcategory'])  # Only pass the categorical features used for one-hot encoding\nplt.figure(figsize=(10, 5))\nplt.barh(feature_names_onehot, feature_importances_onehot)\nplt.title('Feature Importance (One-Hot Encoding)')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.savefig('/kaggle/working/feature_importance_onehot.png')\nplt.show()\n\n# Feature importance for label encoded data\nfeature_importances_label = best_label.feature_importances_\nfeature_names_label = ['Category_LabelEncoded', 'Subcategory_LabelEncoded', 'NumericalFeature1', 'NumericalFeature2']\nplt.figure(figsize=(10, 5))\nplt.barh(feature_names_label, feature_importances_label)\nplt.title('Feature Importance (Label Encoding)')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.savefig('/kaggle/working/feature_importance_label.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-10T14:07:54.172427Z","iopub.execute_input":"2024-06-10T14:07:54.173102Z","iopub.status.idle":"2024-06-10T14:07:54.906002Z","shell.execute_reply.started":"2024-06-10T14:07:54.173039Z","shell.execute_reply":"2024-06-10T14:07:54.904899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary of Findings:\n# Model Performance:\n\nInitial accuracy with one-hot encoding: 0.3667\nInitial accuracy with label encoding: 0.4667\nLabel encoding outperformed one-hot encoding in this dataset.\nNext Steps:\n\nTuned hyperparameters using GridSearchCV.\nBest model cross-validated accuracy:\nOne-Hot Encoding: [Insert Value]\nLabel Encoding: [Insert Value]\nFeature Importance:\n\nVisualized feature importance for both encoding strategies.\nProvided insights into the impact of features on the models.\nConclusion:\nEncoding Strategy:\n\nChoice of encoding strategy influences model performance.\nLabel encoding performed better initially but further tuning may improve one-hot encoding.\nFuture Work:\n\nExperiment with other models and advanced encoding techniques.\nExplore feature engineering and model interpretability.\n","metadata":{}}]}